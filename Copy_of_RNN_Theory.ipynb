{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of RNN_Theory.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ikyath/RNN_Introduction_Fundamentals/blob/master/Copy_of_RNN_Theory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLyGaLAlk1Pa",
        "colab_type": "text"
      },
      "source": [
        "# Introduction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hC5R0m-_qEiJ",
        "colab_type": "text"
      },
      "source": [
        "Time Series"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6kY_z_wqQSJ",
        "colab_type": "text"
      },
      "source": [
        "A time series is a series of data points indexed (or listed or graphed) in time order. Most commonly, a time series is a sequence taken at successive equally spaced points in time. Thus it is a sequence of discrete-time data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-Z4S5NDrPXJ",
        "colab_type": "text"
      },
      "source": [
        "Example - Weather\n",
        "\n",
        "Weather is a dynamical System\n",
        "\n",
        "  See : Chaos Theory,Butterfly effect"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gy3xWZoosYZc",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://i.ytimg.com/vi/D6jln7pqn70/maxresdefault.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9C7s7VqsHhR",
        "colab_type": "text"
      },
      "source": [
        "This is that even if we have the exact deterministic equations to describe a weather system our forecast will still become more and more incorrect.\n",
        "\n",
        "The further into the future we try to predict that's pretty counterintuitive because you would think if you have the exact equation for something then you should be able to calculate all the future values precisely but due to the butterfly effect this is not actually true.\n",
        "\n",
        "As the saying goes a butterfly flapping its wings in Tokyo can cause a tornado in America.\n",
        "\n",
        "Small and decisions like numerical round off error and your computer will ultimately lead it to your weather forecast being completely wrong eventually.This is actually really relevant to us because when you think of time series and our own ends you automatically"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyGEAFgxtB5m",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://media.graytvinc.com/images/810*455/lnk+wx1.jpg)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y07WWouiu4Xk",
        "colab_type": "text"
      },
      "source": [
        "Another type of Sequential data is Text but in machine learning we don't consider it as sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pcjzxwj50NbL",
        "colab_type": "text"
      },
      "source": [
        "Forecasting is about to predict the next values in a timeseries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beh-APbe2nwp",
        "colab_type": "text"
      },
      "source": [
        "# RNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnRW4pVZ3LDL",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "*  The brain consists of billion of neurons, without any single duration.\n",
        "*  A Decision made now is not only based only on what you hear/see now.\n",
        "*  We can think and reason based on past inputs.\n",
        "*  What happens if we add feedback loops and memory to neural network.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAxleSxI-SDT",
        "colab_type": "text"
      },
      "source": [
        "Traditional neural networks can’t do this, and it seems like a major shortcoming. For example, imagine you want to classify what kind of event is happening at every point in a movie. It’s unclear how a traditional neural network could use its reasoning about previous events in the film to inform later ones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbtrdIqcLw1-",
        "colab_type": "text"
      },
      "source": [
        "### Problem of Long-term dependencies\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWMQpg1pL47X",
        "colab_type": "text"
      },
      "source": [
        "The basic idea of RNN is connecting the previous information to the present task. But there are scenarios where we would need only the most recent information and other cases where we need a whole lot of old, meaning information needed more context. \n",
        "Let us denote this as  the gap between the relevant information and the place where it is needed. As the gap grows, RNNs will not be able to learn to connect the information.\n",
        "LSTMs do not have this kind of problem and are able to learn the long term dependencies. Let us discuss how"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBOl_mIlaqFa",
        "colab_type": "text"
      },
      "source": [
        "# LSTMs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvgS4pfmasRo",
        "colab_type": "text"
      },
      "source": [
        "LSTMs are special kind of RNNs, designed explicitly to overcome the long term dependency problem. The repeating modules of neural networks in LSTM have a complex structure compared to that of RNN.\n",
        "The repeating module in LSTM has four interacting layers\n",
        "Memory cell\n",
        "\n",
        "*   Memory cell\n",
        "*   Forget gate\n",
        "*   Input \n",
        "*   Output\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qd6QKUqOduiq",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1CWT9mL2FiRRH6Bi9zmA6LF9arAEn2_jW)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiBvIYKXd5ll",
        "colab_type": "text"
      },
      "source": [
        "Now lets look familiarize with the notations \n",
        "![alt text](https://drive.google.com/uc?id=1ZFEzgRdgFu116fZTyy6LIRaab1sLxqUz)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diB_Icdceauc",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "*   Memory/Cell-state - This is represented as the horizontal layer running through the top of the diagram. It has some linear transformations as pointwise multiplication and addition\n",
        "*   Gates - These are used to optionally let information through using sigmoid neural network layers, which outputs in either 0 or 1, representing the information which we want to leave or the information we want to persist.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7LqfLpIxjU8",
        "colab_type": "text"
      },
      "source": [
        "## Walk-through of LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RhPOYSAxtzW",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQ2YCLbW7GzQ",
        "colab_type": "text"
      },
      "source": [
        "# GRUs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhcDfEhl7JfN",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "GRUs are imporved version of standard recurrent neural networks also defined as RNNs which have gated mechanism.\n",
        "*  It was introduced by Kyunghyun Cho in 2014. [Refer Here](https://arxiv.org/abs/1406.1078)\n",
        "*  The GRU is like a long short-term memory (LSTM) with forget gate but has fewer parameters than LSTM, as it lacks an output gate.\n",
        "*   The GRU is the newer generation of Recurrent Neural networks and is pretty similar to an LSTM. GRU’s got rid of the cell state and used the hidden state to transfer information. It also only has two gates, a reset gate and update gate.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tl5pIpEL9Z00",
        "colab_type": "text"
      },
      "source": [
        "#### Architecture of GRUs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZ63eg41aKqI",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://miro.medium.com/max/1400/1*jhi5uOm9PvZfmxvfaCektw.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQKIUPTpauTT",
        "colab_type": "text"
      },
      "source": [
        "Update Gate : First, we have the Update gate. This gate decides what information should be thrown away or kept. Information from the previous hidden state and information from the current input is passed through the sigmoid function. Values come out between 0 and 1. The closer to 0 means to forget, and the closer to 1 means to keep."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjNu1M1rbIAU",
        "colab_type": "text"
      },
      "source": [
        "Reset Gate :\n",
        "The reset gate is another gate is used to decide how much past information to forget."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNLFFXqBaSxS",
        "colab_type": "text"
      },
      "source": [
        "# References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrchHtQoadfV",
        "colab_type": "text"
      },
      "source": [
        "https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eq3qHGc7aeIK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}